{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 10` - Introduction to Neural Networks\n",
    "\n",
    "In this workshop we provide a very short introduction to neural networks in Python. This is very far from a comprehensive coverage of the topic but can provide a quick start for those who wish to learn more about the topic in their own time. We will cover a classification and a regression task using `keras` as our python package of choice. If you want to try and implement a NN from scratch, there are several good online tutorials that can help you do so (see [here](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) for example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological inspiration\n",
    "The (for our purpose) smallest stand-alone element in the human brain is the neuron. Its understanding and computational recreation build the foundation for ANNs. A simplified image of a \"real\" neuron can be seen below:\n",
    "\n",
    "![](bio_neuron.png)\n",
    "\n",
    "Dendrites are connecting to the axons (or \"outputs\") of other neurons, for instance, nerves in the sensory system or other processing neurons. In the nucleus, these input signals are aggregated and forwarded through the axon. The axon terminals then connect to further neurons to build the neural network. The connection between axon terminal and dendrite is what we are calling a synapse. In the human brain, there are billions of neurons and $10^{14} - 10^{15}$ synapses in the human brain. If each synapse (or more precisely, its connection strength) would be represented by 8 bits or one byte, just storing these numbers would take 1000 TB already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational implementation\n",
    "To recreate neural networks artificially, neurons have to be defined. The common mathematical model used for this purpose is depicted below:\n",
    "\n",
    "![](math_neuron.jpeg)\n",
    "\n",
    "From a certain number of input synapses $x_i$, signals come in with a weight factor of $w_i$. This represents the strength of the synapse. In the _nucleus_ these weighted inputs are aggregated and a bias is added. The bias is not explicitly shown in every model, but it does make the neural network more generalizable. After adding of the weighted inputs and the bias, everything is fed into a (non-linear) activation function. The output is then either fed forward to further neurons or is the output of your neural network. If there is only one neuron that takes direct inputs and whose output is your interest, the model is called a single-layer perceptron. Many of these neurons can create almost arbitrary logical connections and functions, making ANNs very powerful. In this case, we are talking about a multi-layer perceptron (MLP) model: \n",
    "\n",
    "![](mlp-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "The activation function is (to some degree) the heart of the neural network. Without a non-linear activation function, all hidden layers do not add any value, but are instead a complicated way to represent a liner model. Only with a non-linear activation function, ANNs can recreate non-linear hypothesis functions. In the beginning of research on the ANNs in the scope of AI, typically a unit step was used as an activation function. The unit step is $0$ for inputs smaller than $0$ and $1$ otherwise. The idea behind this is to recreate the behavior of a biological neuron that _fires_ if a certain threshold of inputs is exceeded. Today, other activation functions are more commonly used. This is linked to better mathematical qualities in terms of learning behavior and convergence. Some of the most popular activation functions are:\n",
    "\n",
    "Sigmoid: $\\sigma(z) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "Hyperbolic tangent: $\\sigma(z) = \\frac{2}{1+exp(-2z)} -1 $\n",
    "\n",
    "ReLU (Rectified Linear Unit): $\\sigma(z) = z\\quad  for\\ z>0,\\ 0\\ otherwise$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "As learning of ANNs is a non-trivial mathematical task, we are only aiming for an intuitive understanding here. Let's have a look at our complete MLP first.\n",
    "\n",
    "The general learning task consists of two steps, which are repeated until the algorithm converges:\n",
    "1. __Feedforward: Calculating the predicted output ŷ and the associated loss__. At first, we randomly assign values for the weights (and the biases). Based on the input features, the output value is calculated.\n",
    "2. __Backpropagation: Updating the weights W and biases b__. If the output value and the target value differ, the weights and biases are updated. To do this, it is calculated how much each weight and bias contributes to the error. Proportionally to this, they are then corrected (scaled with a small learning factor). In this sense, the updating rule has some similarity to gradient descent, only that is is propagated through the entire network, which is why this algorithm is called backpropagation.\n",
    "\n",
    "The training routine for a simple 2-layered MLP is shown in the below figure:\n",
    "\n",
    "![](training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The main hyperparameters of an MLP are: \n",
    "\n",
    "1. Number of hidden layers\n",
    "1. Number of nodes\n",
    "4. Activation function\n",
    "\n",
    "The more layers and nodes there are (and the denser the network is, i.e. the more edges have a non-zero weight) the harder it gets to learn the model. That's the reason why bigger ANNs are normally not trained on a local computer anymore, but on specialized computers. Furthermore, there are additional libraries for python to improve the efficiency of ANNs, e.g. TensorFlow or Keras, which we take a first look at in today's tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` is one of the most popular Deep Learning libraries. `PyTorch` and `Jax` are the most used numerical platforms in Python to build Deep Learning algorithms but they can be quite complex and difficult to use.\n",
    "\n",
    "Keras, by contrast, is easy to use and is capable of running on top of multiple low-level tensor operation frameworks. The full documentation of the keras API can be found [here](https://keras.io).\n",
    "\n",
    "Note that `scikit learn` also features an MLP implementation (see [here](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)). Yet, `keras` has advanced to be one of the most popular frameworks used in practice, which is why we focus on it in this short tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Keras` to command `PyTorch`, therefore we need to install both.\n",
    "\n",
    "PyTorch's installation method varies by platform and environment manager, all of the options are listed [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "If you use the provided `environment.yml` specification, you should be set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks for classification in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay with our example, we will build a NN that predicts the class of a breast cancer cell by categorizing it as either malignant or benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# supress versioning warnings of keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because keras can work with multiple different backends, it is important to specify that we want to use PyTorch before importing keras for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential # sequential model: https://keras.io/guides/sequential_model/\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                       \n",
       "842302         M        17.99         10.38           122.8     1001.0   \n",
       "842517         M        20.57         17.77           132.9     1326.0   \n",
       "\n",
       "        smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                          \n",
       "842302          0.11840           0.27760          0.3001   \n",
       "842517          0.08474           0.07864          0.0869   \n",
       "\n",
       "        concave points_mean  symmetry_mean  ...  radius_worst  texture_worst  \\\n",
       "id                                          ...                                \n",
       "842302              0.14710         0.2419  ...         25.38          17.33   \n",
       "842517              0.07017         0.1812  ...         24.99          23.41   \n",
       "\n",
       "        perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "id                                                                         \n",
       "842302            184.6      2019.0            0.1622             0.6656   \n",
       "842517            158.8      1956.0            0.1238             0.1866   \n",
       "\n",
       "        concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "id                                                              \n",
       "842302           0.7119                0.2654          0.4601   \n",
       "842517           0.2416                0.1860          0.2750   \n",
       "\n",
       "        fractal_dimension_worst  \n",
       "id                               \n",
       "842302                  0.11890  \n",
       "842517                  0.08902  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and Y\n",
    "X = cancer_df.iloc[:,1:31] # include full feature vector\n",
    "y = cancer_df[\"diagnosis\"]\n",
    "\n",
    "\n",
    "# encode categorical target vector\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing and Training the ANN\n",
    "\n",
    "We start by defining the type of model we want to build. There are two types of models available in Keras: the [Sequential model](https://keras.io/models/sequential/) and the model class used with [functional API](https://keras.io/models/model/). Then, we simply add the input-, hidden- and output-layers.\n",
    "\n",
    "Between them, we are using [dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) to prevent overfitting (dropout rate should be between 20% and 50%).\n",
    "\n",
    "![](dropout.png)\n",
    "\n",
    "At every layer, we use “Dense” which means that the nodes are fully connected.\n",
    "\n",
    "In our example, the input-layer takes 30 inputs (because our feature vector includes 30 features) and outputs it with a shape of 15, which is the number of nodes in the first hidden layer that we define. Then, we define a second hidden layer with 15 nodes, before adding the output layer with a single node (since we are solving a binary classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the following parameters:\n",
    "\n",
    "- input_shape - number of columns of the dataset (only for input layer)\n",
    "\n",
    "- units - number of neurons and dimensionality of outputs to be fed to the next layer, if any\n",
    "\n",
    "- activation - activation function (we use ReLU in this case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the input layer and the first hidden layer (with 15 nodes)\n",
    "classifier.add(Dense(input_shape = (30,), \n",
    "                     units=15,          # dimensionality of the output space (# of nodes in the first hidden layer)\n",
    "                     activation='relu'))\n",
    "\n",
    "# adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional second layer, also with 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the second hidden layer\n",
    "classifier.add(Dense(units= 15,\n",
    "                     activation='relu'))\n",
    "\n",
    "# adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the output layer. Since we perform a binary classification, a single output node suffices. We use a sigmoidal activation function for this last node which is often used when dealing with binary classfication problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the output layer\n",
    "classifier.add(Dense(units= 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model to configure it for training. We add the following parameters:\n",
    "- `optimizer`: Here we use the adam optimizer, an optimizer with higher performance in many cases than stochastic gradient descent (SGD). See [here](https://keras.io/optimizers/) for a list of all optimizers implemented in `keras`.\n",
    "- `loss`: specifies the loss to be minimized. In this example we use binary cross-entropy, a common loss for binary classification tasks. See [here](https://keras.io/losses/) for an overview of available losses in `keras`.\n",
    "- `metrics`: the metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model and merely function as an indicator of model performance to the data scientist. In this example we report the accuracy. An overview ov available metrics can be found [here](https://keras.io/metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the ANN\n",
    "classifier.compile(optimizer=\"adam\",    # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\n",
    "              loss=\"binary_crossentropy\",  # this is a good loss for binary classification\n",
    "              metrics=[\"accuracy\"]) # standard classification evaluation emtric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">465</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m465\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">721</span> (2.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m721\u001b[0m (2.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">721</span> (2.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m721\u001b[0m (2.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to train our model. We do this with a batch_size of 50 and for 100 epochs.\n",
    "\n",
    "- `batch_size` defines the number of samples that will be propagated through the network \n",
    "- `epoch` defines the number of iterations over the entire training data\n",
    "\n",
    "In general a larger batch-size results in faster training, but does not always converge fast. A smaller batch-size is slower in training but it can converge faster. This is definitely problem-dependent and you need to try out a few different values (the standard batch-size is 32). The same goes for the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6884 - loss: 0.6050 \n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7337 - loss: 0.5259\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7915 - loss: 0.4619\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8065 - loss: 0.4506\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8442 - loss: 0.3991\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8693 - loss: 0.3824\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8719 - loss: 0.3707\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8719 - loss: 0.3311\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8995 - loss: 0.3152\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8945 - loss: 0.2975\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8995 - loss: 0.2775\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9271 - loss: 0.2641\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9146 - loss: 0.2473\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9171 - loss: 0.2265\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9171 - loss: 0.2246\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.1917\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9322 - loss: 0.1936\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9447 - loss: 0.1803\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9397 - loss: 0.1874\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1591\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.1656\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.1624\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1484\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.1508\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1483\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9548 - loss: 0.1216\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1143\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1364\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1193\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1095\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1256\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1270\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1091\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.1020\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1130\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1095\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.1196\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.1075\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1086\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0958\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.1017\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.0997\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0834\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0852\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0947\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0933\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.0917\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0950\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.0906\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.0870\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0835\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0735\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0769\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0636\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0660\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0768\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0743\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0821\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0866\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0787\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0782\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0767\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0722\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0692\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0744\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0775\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0807\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9749 - loss: 0.0701 \n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0581\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0701\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0787\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9824 - loss: 0.0594\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0610\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0635\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0681\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0639\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9824 - loss: 0.0593\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0687\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0639\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0584\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0616\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9824 - loss: 0.0584\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.0571\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0528\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0639\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0470\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.0414\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.0474\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0596\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0595\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0544\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0572\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0475\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.0463\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0594\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0459\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9824 - loss: 0.0607\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0585\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0579\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dc3f546a10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size=50, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Confusion Matrix\n",
      "[[107   1]\n",
      " [  2  61]]\n",
      "\n",
      "Accuracy\n",
      "0.9825\n",
      "\n",
      "Precision\n",
      "0.9839\n"
     ]
    }
   ],
   "source": [
    "# report classification performance on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "accuracy_score = accuracy_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "precision_score = precision_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print(\"Accuracy\")\n",
    "print(round(accuracy_score, ndigits=4))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(round(precision_score, ndigits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks for regression in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also be trained for regression tasks. The logic is exactly the same, yet some of the parameters, such as loss, metrics, input and ouput as well as typical activation functions might have to be adapted to the specific case. There are a range of very good tutorials online, which we encourage you to take a look at (for example [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)). \n",
    "\n",
    "We will cover a simple implimentation on the `Diamonds` dataset. The objective in this task is to predict the price of a particular diamond based on different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>1.16</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6036</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.75</td>\n",
       "      <td>4.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35973</th>\n",
       "      <td>0.32</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>IF</td>\n",
       "      <td>61.7</td>\n",
       "      <td>55.0</td>\n",
       "      <td>921</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32637</th>\n",
       "      <td>0.25</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VS2</td>\n",
       "      <td>61.8</td>\n",
       "      <td>58.0</td>\n",
       "      <td>459</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.08</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45173</th>\n",
       "      <td>0.57</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>61.6</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1655</td>\n",
       "      <td>5.35</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>0.73</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>VVS1</td>\n",
       "      <td>61.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3990</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.78</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "14982   1.16      Ideal     H     SI1   62.2   54.0   6036  6.72  6.75  4.19\n",
       "35973   0.32  Very Good     F      IF   61.7   55.0    921  4.40  4.41  2.72\n",
       "32637   0.25      Ideal     G     VS2   61.8   58.0    459  4.04  4.08  2.51\n",
       "45173   0.57      Ideal     H     VS2   61.6   57.0   1655  5.35  5.37  3.30\n",
       "6157    0.73      Ideal     F    VVS1   61.8   54.0   3990  5.80  5.78  3.58"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data\n",
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# no missing values\n",
    "fig, ax = plt.subplots(1,1)\n",
    "sns.heatmap(diamonds.isna(), ax=ax,\n",
    "           vmin=0, vmax=1, cmap=\"Reds\",\n",
    "           cbar_kws={\"ticks\":[0,1]})\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Show Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dummy variables\n",
    "\n",
    "Since in the diamond dataset we have three categorical input features, we need to convert them into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>...</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_I1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>1.08</td>\n",
       "      <td>61.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8405</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.69</td>\n",
       "      <td>4.08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17013</th>\n",
       "      <td>1.03</td>\n",
       "      <td>60.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6793</td>\n",
       "      <td>6.53</td>\n",
       "      <td>6.56</td>\n",
       "      <td>3.97</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26899</th>\n",
       "      <td>2.05</td>\n",
       "      <td>60.8</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16896</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>61.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2043</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.67</td>\n",
       "      <td>3.53</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>0.37</td>\n",
       "      <td>59.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>684</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.80</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  depth  table  price     x     y     z  cut_Ideal  cut_Premium  \\\n",
       "19845   1.08   61.4   55.0   8405  6.61  6.69  4.08       True        False   \n",
       "17013   1.03   60.7   57.0   6793  6.53  6.56  3.97       True        False   \n",
       "26899   2.05   60.8   58.0  16896  8.26  8.19  5.00      False         True   \n",
       "48936   0.72   61.7   58.0   2043  5.77  5.67  3.53      False         True   \n",
       "28868   0.37   59.7   56.0    684  4.71  4.67  2.80       True        False   \n",
       "\n",
       "       cut_Very Good  ...  color_I  color_J  clarity_IF  clarity_VVS1  \\\n",
       "19845          False  ...    False    False       False         False   \n",
       "17013          False  ...    False    False       False         False   \n",
       "26899          False  ...     True    False       False         False   \n",
       "48936          False  ...    False     True       False         False   \n",
       "28868          False  ...    False    False       False         False   \n",
       "\n",
       "       clarity_VVS2  clarity_VS1  clarity_VS2  clarity_SI1  clarity_SI2  \\\n",
       "19845         False         True        False        False        False   \n",
       "17013         False        False         True        False        False   \n",
       "26899         False        False         True        False        False   \n",
       "48936         False        False         True        False        False   \n",
       "28868         False        False        False         True        False   \n",
       "\n",
       "       clarity_I1  \n",
       "19845       False  \n",
       "17013       False  \n",
       "26899       False  \n",
       "48936       False  \n",
       "28868       False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate dummies for categorical features\n",
    "diamonds = pd.get_dummies(diamonds)\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining inputs and output\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing training data\n",
    "st_scaler = StandardScaler()\n",
    "st_scaler.fit(X_train)\n",
    "X_train_scaled = st_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "model = Sequential(\n",
    "    [Dense(36, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    Dense(36, activation=\"relu\"),\n",
    "     Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "model.compile(loss='mse',\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"mae\", \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">972</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,332</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │           \u001b[38;5;34m972\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m1,332\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,341</span> (9.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,341\u001b[0m (9.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,341</span> (9.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,341\u001b[0m (9.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 16830278.0000 - mae: 2630.4351 - mse: 16830278.0000 - val_loss: 2637232.2500 - val_mae: 1122.0835 - val_mse: 2637232.2500\n",
      "Epoch 2/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1753633.1250 - mae: 808.3733 - mse: 1753633.1250 - val_loss: 885631.9375 - val_mae: 634.7272 - val_mse: 885631.9375\n",
      "Epoch 3/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1153175.7500 - mae: 620.2828 - mse: 1153175.7500 - val_loss: 763919.7500 - val_mae: 588.0011 - val_mse: 763919.7500\n",
      "Epoch 4/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1071233.8750 - mae: 592.4647 - mse: 1071233.8750 - val_loss: 716674.1875 - val_mae: 566.4540 - val_mse: 716674.1875\n",
      "Epoch 5/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1013067.0000 - mae: 565.6961 - mse: 1013067.0000 - val_loss: 680875.8125 - val_mae: 538.5183 - val_mse: 680875.8125\n",
      "Epoch 6/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 963106.0000 - mae: 540.7073 - mse: 963106.0000 - val_loss: 650186.6250 - val_mae: 513.5146 - val_mse: 650186.6250\n",
      "Epoch 7/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 918561.5000 - mae: 518.6404 - mse: 918561.5000 - val_loss: 625834.6875 - val_mae: 490.1949 - val_mse: 625834.6875\n",
      "Epoch 8/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 879266.5625 - mae: 497.5836 - mse: 879266.5625 - val_loss: 607367.9375 - val_mae: 472.9302 - val_mse: 607367.9375\n",
      "Epoch 9/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 843505.3125 - mae: 479.9744 - mse: 843505.3125 - val_loss: 579548.3125 - val_mae: 462.8629 - val_mse: 579548.3125\n",
      "Epoch 10/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 813804.8125 - mae: 466.6990 - mse: 813804.8125 - val_loss: 559248.3125 - val_mae: 445.9179 - val_mse: 559248.3125\n",
      "Epoch 11/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 783695.5625 - mae: 452.5737 - mse: 783695.5625 - val_loss: 546571.4375 - val_mae: 435.9978 - val_mse: 546571.4375\n",
      "Epoch 12/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 757819.9375 - mae: 441.7332 - mse: 757819.9375 - val_loss: 572523.8750 - val_mae: 433.7186 - val_mse: 572523.8750\n",
      "Epoch 13/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 733889.5625 - mae: 430.1665 - mse: 733889.5625 - val_loss: 518414.2500 - val_mae: 412.1901 - val_mse: 518414.2500\n",
      "Epoch 14/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 711284.0625 - mae: 421.2128 - mse: 711284.0625 - val_loss: 504069.6875 - val_mae: 403.8496 - val_mse: 504069.6875\n",
      "Epoch 15/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 692113.8750 - mae: 412.0416 - mse: 692113.8750 - val_loss: 494006.6562 - val_mae: 396.5874 - val_mse: 494006.6562\n",
      "Epoch 16/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 672167.1875 - mae: 404.5850 - mse: 672167.1875 - val_loss: 484778.8750 - val_mae: 390.8941 - val_mse: 484778.8750\n",
      "Epoch 17/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 655251.3750 - mae: 398.9918 - mse: 655251.3750 - val_loss: 478262.5312 - val_mae: 382.1327 - val_mse: 478262.5312\n",
      "Epoch 18/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 639794.3750 - mae: 392.1701 - mse: 639794.3750 - val_loss: 478604.8125 - val_mae: 381.7707 - val_mse: 478604.8125\n",
      "Epoch 19/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 625840.0625 - mae: 387.4082 - mse: 625840.0625 - val_loss: 461204.6250 - val_mae: 372.5359 - val_mse: 461204.6250\n",
      "Epoch 20/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 609313.8125 - mae: 383.1861 - mse: 609313.8125 - val_loss: 456200.3750 - val_mae: 368.9665 - val_mse: 456200.3750\n"
     ]
    }
   ],
   "source": [
    "# train the ANN\n",
    "epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train.values,\n",
    "                   epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9290.492  ],\n",
       "       [  659.83014],\n",
       "       [10287.997  ],\n",
       "       [ 2725.4658 ],\n",
       "       [ 3275.4062 ],\n",
       "       [  790.41077],\n",
       "       [ 6833.912  ],\n",
       "       [ 4255.999  ],\n",
       "       [ 7545.356  ],\n",
       "       [ 2435.0159 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions\n",
    "model.predict(X_train_scaled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.683028e+07</td>\n",
       "      <td>2630.435059</td>\n",
       "      <td>1.683028e+07</td>\n",
       "      <td>2.637232e+06</td>\n",
       "      <td>1122.083496</td>\n",
       "      <td>2.637232e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.753633e+06</td>\n",
       "      <td>808.373291</td>\n",
       "      <td>1.753633e+06</td>\n",
       "      <td>8.856319e+05</td>\n",
       "      <td>634.727234</td>\n",
       "      <td>8.856319e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.153176e+06</td>\n",
       "      <td>620.282776</td>\n",
       "      <td>1.153176e+06</td>\n",
       "      <td>7.639198e+05</td>\n",
       "      <td>588.001099</td>\n",
       "      <td>7.639198e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.071234e+06</td>\n",
       "      <td>592.464722</td>\n",
       "      <td>1.071234e+06</td>\n",
       "      <td>7.166742e+05</td>\n",
       "      <td>566.454041</td>\n",
       "      <td>7.166742e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.013067e+06</td>\n",
       "      <td>565.696106</td>\n",
       "      <td>1.013067e+06</td>\n",
       "      <td>6.808758e+05</td>\n",
       "      <td>538.518311</td>\n",
       "      <td>6.808758e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.631060e+05</td>\n",
       "      <td>540.707336</td>\n",
       "      <td>9.631060e+05</td>\n",
       "      <td>6.501866e+05</td>\n",
       "      <td>513.514648</td>\n",
       "      <td>6.501866e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.185615e+05</td>\n",
       "      <td>518.640442</td>\n",
       "      <td>9.185615e+05</td>\n",
       "      <td>6.258347e+05</td>\n",
       "      <td>490.194916</td>\n",
       "      <td>6.258347e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.792666e+05</td>\n",
       "      <td>497.583557</td>\n",
       "      <td>8.792666e+05</td>\n",
       "      <td>6.073679e+05</td>\n",
       "      <td>472.930206</td>\n",
       "      <td>6.073679e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.435053e+05</td>\n",
       "      <td>479.974365</td>\n",
       "      <td>8.435053e+05</td>\n",
       "      <td>5.795483e+05</td>\n",
       "      <td>462.862915</td>\n",
       "      <td>5.795483e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.138048e+05</td>\n",
       "      <td>466.699005</td>\n",
       "      <td>8.138048e+05</td>\n",
       "      <td>5.592483e+05</td>\n",
       "      <td>445.917908</td>\n",
       "      <td>5.592483e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.836956e+05</td>\n",
       "      <td>452.573669</td>\n",
       "      <td>7.836956e+05</td>\n",
       "      <td>5.465714e+05</td>\n",
       "      <td>435.997833</td>\n",
       "      <td>5.465714e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.578199e+05</td>\n",
       "      <td>441.733246</td>\n",
       "      <td>7.578199e+05</td>\n",
       "      <td>5.725239e+05</td>\n",
       "      <td>433.718597</td>\n",
       "      <td>5.725239e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.338896e+05</td>\n",
       "      <td>430.166534</td>\n",
       "      <td>7.338896e+05</td>\n",
       "      <td>5.184142e+05</td>\n",
       "      <td>412.190094</td>\n",
       "      <td>5.184142e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.112841e+05</td>\n",
       "      <td>421.212769</td>\n",
       "      <td>7.112841e+05</td>\n",
       "      <td>5.040697e+05</td>\n",
       "      <td>403.849609</td>\n",
       "      <td>5.040697e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.921139e+05</td>\n",
       "      <td>412.041626</td>\n",
       "      <td>6.921139e+05</td>\n",
       "      <td>4.940067e+05</td>\n",
       "      <td>396.587433</td>\n",
       "      <td>4.940067e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.721672e+05</td>\n",
       "      <td>404.585022</td>\n",
       "      <td>6.721672e+05</td>\n",
       "      <td>4.847789e+05</td>\n",
       "      <td>390.894104</td>\n",
       "      <td>4.847789e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.552514e+05</td>\n",
       "      <td>398.991760</td>\n",
       "      <td>6.552514e+05</td>\n",
       "      <td>4.782625e+05</td>\n",
       "      <td>382.132721</td>\n",
       "      <td>4.782625e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.397944e+05</td>\n",
       "      <td>392.170074</td>\n",
       "      <td>6.397944e+05</td>\n",
       "      <td>4.786048e+05</td>\n",
       "      <td>381.770691</td>\n",
       "      <td>4.786048e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.258401e+05</td>\n",
       "      <td>387.408203</td>\n",
       "      <td>6.258401e+05</td>\n",
       "      <td>4.612046e+05</td>\n",
       "      <td>372.535919</td>\n",
       "      <td>4.612046e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.093138e+05</td>\n",
       "      <td>383.186096</td>\n",
       "      <td>6.093138e+05</td>\n",
       "      <td>4.562004e+05</td>\n",
       "      <td>368.966461</td>\n",
       "      <td>4.562004e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss          mae           mse      val_loss      val_mae  \\\n",
       "0   1.683028e+07  2630.435059  1.683028e+07  2.637232e+06  1122.083496   \n",
       "1   1.753633e+06   808.373291  1.753633e+06  8.856319e+05   634.727234   \n",
       "2   1.153176e+06   620.282776  1.153176e+06  7.639198e+05   588.001099   \n",
       "3   1.071234e+06   592.464722  1.071234e+06  7.166742e+05   566.454041   \n",
       "4   1.013067e+06   565.696106  1.013067e+06  6.808758e+05   538.518311   \n",
       "5   9.631060e+05   540.707336  9.631060e+05  6.501866e+05   513.514648   \n",
       "6   9.185615e+05   518.640442  9.185615e+05  6.258347e+05   490.194916   \n",
       "7   8.792666e+05   497.583557  8.792666e+05  6.073679e+05   472.930206   \n",
       "8   8.435053e+05   479.974365  8.435053e+05  5.795483e+05   462.862915   \n",
       "9   8.138048e+05   466.699005  8.138048e+05  5.592483e+05   445.917908   \n",
       "10  7.836956e+05   452.573669  7.836956e+05  5.465714e+05   435.997833   \n",
       "11  7.578199e+05   441.733246  7.578199e+05  5.725239e+05   433.718597   \n",
       "12  7.338896e+05   430.166534  7.338896e+05  5.184142e+05   412.190094   \n",
       "13  7.112841e+05   421.212769  7.112841e+05  5.040697e+05   403.849609   \n",
       "14  6.921139e+05   412.041626  6.921139e+05  4.940067e+05   396.587433   \n",
       "15  6.721672e+05   404.585022  6.721672e+05  4.847789e+05   390.894104   \n",
       "16  6.552514e+05   398.991760  6.552514e+05  4.782625e+05   382.132721   \n",
       "17  6.397944e+05   392.170074  6.397944e+05  4.786048e+05   381.770691   \n",
       "18  6.258401e+05   387.408203  6.258401e+05  4.612046e+05   372.535919   \n",
       "19  6.093138e+05   383.186096  6.093138e+05  4.562004e+05   368.966461   \n",
       "\n",
       "         val_mse  \n",
       "0   2.637232e+06  \n",
       "1   8.856319e+05  \n",
       "2   7.639198e+05  \n",
       "3   7.166742e+05  \n",
       "4   6.808758e+05  \n",
       "5   6.501866e+05  \n",
       "6   6.258347e+05  \n",
       "7   6.073679e+05  \n",
       "8   5.795483e+05  \n",
       "9   5.592483e+05  \n",
       "10  5.465714e+05  \n",
       "11  5.725239e+05  \n",
       "12  5.184142e+05  \n",
       "13  5.040697e+05  \n",
       "14  4.940067e+05  \n",
       "15  4.847789e+05  \n",
       "16  4.782625e+05  \n",
       "17  4.786048e+05  \n",
       "18  4.612046e+05  \n",
       "19  4.562004e+05  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect metrics by epoch\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4102.472182</td>\n",
       "      <td>1623.955741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1324.248136</td>\n",
       "      <td>941.080197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073.860210</td>\n",
       "      <td>874.025028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035.004287</td>\n",
       "      <td>846.566115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1006.512295</td>\n",
       "      <td>825.151994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>981.379641</td>\n",
       "      <td>806.341506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>958.416141</td>\n",
       "      <td>791.097142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>937.692147</td>\n",
       "      <td>779.338141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>918.425453</td>\n",
       "      <td>761.280705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>902.111308</td>\n",
       "      <td>747.829066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>885.265815</td>\n",
       "      <td>739.304699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>870.528539</td>\n",
       "      <td>756.653074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>856.673545</td>\n",
       "      <td>720.009896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>843.376584</td>\n",
       "      <td>709.978653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>831.933816</td>\n",
       "      <td>702.856071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>819.858029</td>\n",
       "      <td>696.260637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>809.475988</td>\n",
       "      <td>691.565276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>799.871474</td>\n",
       "      <td>691.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>791.100539</td>\n",
       "      <td>679.120479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>780.585557</td>\n",
       "      <td>675.426069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse     val_rmse\n",
       "0   4102.472182  1623.955741\n",
       "1   1324.248136   941.080197\n",
       "2   1073.860210   874.025028\n",
       "3   1035.004287   846.566115\n",
       "4   1006.512295   825.151994\n",
       "5    981.379641   806.341506\n",
       "6    958.416141   791.097142\n",
       "7    937.692147   779.338141\n",
       "8    918.425453   761.280705\n",
       "9    902.111308   747.829066\n",
       "10   885.265815   739.304699\n",
       "11   870.528539   756.653074\n",
       "12   856.673545   720.009896\n",
       "13   843.376584   709.978653\n",
       "14   831.933816   702.856071\n",
       "15   819.858029   696.260637\n",
       "16   809.475988   691.565276\n",
       "17   799.871474   691.812700\n",
       "18   791.100539   679.120479\n",
       "19   780.585557   675.426069"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate RMSE for better interpretability\n",
    "root_metrics_df = history_df[[\"mse\", \"val_mse\"]].apply(np.sqrt)\n",
    "root_metrics_df.rename({\"mse\":\"rmse\", \"val_mse\":\"val_rmse\"}, axis=1, inplace=True)\n",
    "root_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure(figsize=(14,6), dpi=100)\n",
    "\n",
    "plt.plot(root_metrics_df[\"rmse\"], label = 'Training error')\n",
    "plt.plot(root_metrics_df[\"val_rmse\"], label = 'Validation error')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "# plt.xlim([0, epochs])\n",
    "plt.xticks(range(1,20))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step\n"
     ]
    }
   ],
   "source": [
    "# generate predictions on test set\n",
    "X_test_scaled = st_scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  691.6688830285775\n",
      "MAE:  371.6260681152344\n"
     ]
    }
   ],
   "source": [
    "# report performance on test set\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"MAE: \", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
